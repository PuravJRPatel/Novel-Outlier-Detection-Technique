# Novel-Outlier-Detection-using-Stacking-Ensemble-Approach

### Abstract
_A significant number of methods have been devised for anomaly detection. Traditional methods like LOF, IForest, CBLOF, etc. have a strong performance and simple methodologies, making them very popular. However, the recently proposed methods of LUNAR and ECOD trump them with their better performance, where ECOD is a simpler, faster, and more interpretable method with a profound ability to detect global anomalies, and on the other hand, LUNAR introduces the trainability of parameters as a tremendous advantage and unifies the local outlier detection methods. These methods allow for the introduction of a new ensemble method that utilises LUNAR and ECOD as base models, combined with Logistic regression as a meta model. This study introduces a technique that surpasses the original foundational techniques and other widely used outlier detection methods. It consistently produces reliable outcomes compared to other algorithms when applied to the specific dataset._

### Introduction
Outliers, often known as anomalies, are points of information that display distinct characteristics in comparison to the rest of the dataset. Their detection is important because they are shown to affect the analysis process. Outliers can be characterised as either global anomalies or local anomalies. Global anomalies are points of information that behave differently from the entire dataset, whereas local anomalies are areas that differ substantially from the points closer to them, i.e., their neighbours, but seem to blend in with the entire dataset. 
Anomaly detection has multiple applications spanning across many domains, such as fraud detection, health, cybersecurity, equipment monitoring, et. al. Hence, the involvement of real-life data led to the introduction of multiple unsupervised anomaly detection methods such as Local Outlier Factor, Connectivity-Based Outlier Factor, Isolation Forest, Angle-Based Outlier Detection, Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions (ECOD) and LUNAR: Unifying Local Outlier Detection Methods via Graph Neural Networks, etc.
The aforementioned include a handful of techniques that have been developed over the past two decades. They follow different approaches, like proximity, probability, and graph-based methods, et al., for anomaly detection. Out of these techniques, ECOD and LUNAR are recently developed unsupervised algorithms that are probabilistic and graph-based, respectively. ECOD is based on the fact that outliers are rare events that appear at one of the tails of the data distribution, making it an outstanding method for global anomaly detection. LUNAR is a graph-based method that unifies other local outlier methods under a framework based on graph neural networks. The inclusion of these methods, along with deep learning-based methods, makes it an excellent local anomaly detection technique.
Hence, in this research, a new ensemble method that contains the merits of the two algorithms, ECOD and LUNAR, is proposed. In, the authors describe ECOD as a fast, efficient, understandable, and non-parametric approach to the detection of outliers, but the latter lacks the ability to work with outliers that are mixed with the rest of the data to a certain extent. The proposed method involves combining ECOD and LUNAR in order to leverage the benefits of both approaches. The reason for this is that LUNAR has the ability to integrate various local outlier methods, which aids in the detection of locally anomalous data points, unlike ECOD.
Thus, this ensemble attempts to leverage the probabilistic nature of ECOD and the proposed graphical neural network used in LUNAR for both global and local outlier detection. With regard to the comparison of ECOD and LUNAR methods, ECOD is relatively easy and efficient for the detection of global outliers, while LUNAR is relatively stable and effective in the detection of local outliers. 

### ECOD
According to Li and Zhao, an empirical cumulative distribution function (ECDF) of the data approximates the data distribution without any parametric assumption, making parameter tuning obsolete. After calculating a univariate ECDF for each dimension, ECOD estimates the tail probabilities of each dimension for every data point. 
ECOD computes the left-tail and right-tail ECDFs of each dimension and then aggregates them so as to determine the outlier score, where a higher score points to the possibility of the record being an outlier. 
The immense advantage of ECOD is that it is non-parametric. This leads to a huge reduction in computation overhead. It assumes independence amongst the dimensions in a dataset, which theoretically should have a major negative impact and incur highly inaccurate results, but instead, it persists with decent accuracy in such scenarios. A thorough evaluation of ECOD was conducted on thirty real-life datasets and several other synthetic datasets, and it was observed that ECOD gave consistent results across the board but struggled with local outliers because these outliers are able to blend in with the normal dataset and remain undetectable by it.
The proposed method tackles this issue by training the ECOD and LUNAR (an algorithm specialised in local anomaly detection) models separately and leveraging the probabilistic nature of logistic regression to assess the relative importance of the raw scores of ECOD and LUNAR in the classification process. This results in a more accurate anomaly detection process while incurring relatively less computational overhead.

### LUNAR
Hyperparameter settings are crucial to a model’s performance, and some of the popular local outlier detection methods require optimal tuning in order to achieve decent results. When it comes to real-life data, the ability to adapt to the dataset becomes inevitable; this is where these methods fall behind. LUNAR's proposed solution to this problem is to establish a comprehensive framework that unifies local outlier methods based on message passing schemes used in graphical neural networks. The intrinsic generalisation ability of graphical neural networks leads to high accuracy and flexibility, and by enabling learnability [6] via these GNNs, LUNAR is able to identify local anomalies more accurately. 
Graphical Neural Networks, or GNNs, are an extension of neural network techniques used to process information in graph domains. LUNAR utilises a one-layer graph neural network as per a message-passing framework describes the LUNAR methodology as representing data as a graph. A node serves as the representation of each data point in this graph, and directed edges connect a target node to a group of source nodes, which are the samples' closest neighbours. The information from the nearest neighbour node is used to determine the anomaly score of the target node.   
To test the effectiveness of LUNAR compared to other popular local outlier methods, KNN, COF, LOF, and CBLOF were subjected to evaluation, where each algorithm was tested for n neighbours (n clusters for CBLOF) ranging from 10 to 50. The results indicated that the performance of these algorithms was average in comparison to LUNAR. This is due to the high computational intensity required to calculate their ROC and AUC of ROC. Therefore, LUNAR's use is more advantageous than other proximity-based local outlier detection methods.

### Proposed Methodology
As discussed above, LUNAR’s strength lies in its ability to add the property of learnability to the traditional local outlier methods and unify them with a framework using graphical neural networks. ECOD, on the other hand, is a non-parametric method that is very efficient and accurate, although it does not perform well when it comes to local outliers. Hence, an ensemble that aims to bring their strengths together is able to establish a better performance on accuracy, ROC, and AUC under the ROC. 
![Copy of Ensemble - Page 1](https://github.com/user-attachments/assets/83fc775d-ba61-4489-8589-ba24a5c85366)
The dataset depicted in the figure above undergoes label removal and shuffling to facilitate its suitability for unsupervised learning. Unsupervised learning is a training method used when ground truth is unavailable, making it more suitable for anomaly detection. 
This shuffled data is then normalised using the standard scaler from the Python scikit-learn library. StandardScaler uses the z-score normalisation formula in order to make the dataset look like normally distributed data. StandardScaler is sensitive to outliers; hence, it does not remove them from the dataset and does not hinder the process of anomaly detection by ECOD and LUNAR. 
The normalised data is then separately subjected to the two chosen outlier detection methods, LUNAR and ECOD. Both LUNAR and ECOD are the base models of our ensemble, built using the PyOD library, a comprehensive and easy-to-use Pyhon library that provides a plethora of anomaly detection algorithms. The decision_function(X) attribute of the fitted algorithms is then used in this case to obtain the raw outlier score of X of the fitted detector. The raw scores of both LUNAR and ECOD are then merged with the original labels of the dataset that were stripped off at the first stage.
The new dataframe, or labelled data, is subsequently divided into training data and test data. The training data is fitted to the logistic regression algorithm using Scikit-Learn’s LogisticRegression classifier. The training data for the analysis is fitted to the logistic regression algorithm using Scikit-Learn’s LogisticRegression classifier. This model is then applied to the test data, which was prepared earlier for this purpose. The evaluation metrics that have been chosen for performance measures are accuracy, ROC, and AUC under the ROC curve. In addition, the hold-out method with a five-fold cross-validation procedure is utilised to check the efficiency of the ensemble.
The logistic regression forms the basis of the ensemble, known as the meta-model. Logistic regression is one of the most used and reliable techniques for binary classification . It assesses the effect of independent variables on the result by determining the specific contribution of each independent variable. This helps in making sure that the ordinary scores of the base models are provided with the correct weight in the final categorization. This makes it possible to create a model that is quite efficient in the identification of outliers within a given dataset. The next section describes the results obtained while carrying out multiple experiments, comparing the performance of several other outlier detection methods, the two base model methods, and the new ensemble method.

### Results
The metrics used to evaluate the methods are accuracy, ROC, and AUC. The approach for experiments related to proximity-based algorithms included taking the mean of the accuracy and AUC scores over a range of 10, 20, 30, 40, and 50 neighbours or clusters (CBLOF). As shown in the tables below, many instances showed traditional methods being comparable to the ensemble method, but their reliance on parameter tuning made them slower to fit the data. In the case of COF, two datasets did not fit the algorithm due to intensive computational requirements. 
In the case of outlier detection, it is imperative to recognise the importance of ROC and AUC. The receiver operating characteristic (ROC) curve illustrates the correlation between the true positive rate (sensitivity) and the false positive rate (1-specificity). The ROC curve's area under the curve quantifies the model's overall performance. An AUC of 0.5 usually correlates to a performance that imitates a simple guess. The tables below display performance evaluations of the algorithms, with the bold values indicating the performance measure of the best-performing algorithm. 

The experiments showed that the ensemble had the highest accuracy score for the majority of the datasets. The ensemble was more accurate by a factor of 8.94% compared to its base models. It was observed that, when compared to other popular methods (including the base models), which performed at an accuracy usually ranging between 88% and 92%, the ensemble’s average accuracy was 97.37%, a rise of 8.38% compared with the conventional approaches. 
When it comes to the area under the curve of ROC, the ensemble outperforms the base models for all the datasets. The AUC of the ensemble was 10.41% higher than that of its original base models. Although the performance of the other specific traditional methods seems to be higher than that of the ensemble, the computation time required to find the best parameters, be it the number of neighbours or clusters, outweighs the minute improvement in the AUC. Furthermore, it is noteworthy that the traditional methods do not maintain consistency across the board, unlike the ensemble method, which does not have any significant deviation in its performance with respect to other algorithms. The ROC curve area for the ensemble performed better than the other algorithms at an average rate of 10.16%.

### Conclusion
In this research, the proposed novel method is able to hone the strengths of two existing methods: ECOD and LUNAR, which are at their best when dealing with global and local anomalies, respectively. This method makes use of logistic regression on the raw scores of ECOD and LUNAR in order to find the relative importance of each score. Essentially, this method is a stacking ensemble that combines the raw scores of two base models and introduces a probabilistic approach to determine the combined score, which falls between 0 and 1. 
The assessment of nine datasets from the anomaly detection benchmark demonstrated that our approach surpassed all other algorithms in terms of accuracy. It also managed to get an increased average AUC score compared to other methods while maintaining a strong and consistent performance throughout the datasets.



